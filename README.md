# SeKernel_for_LLM_UI
This is the repository for the UI for the SeKernel_for_LLM module

## How to:
- Clone the repo
- Ensure that you have llama-cpp-python installed and running
- Add your model to the `kernel.py` script
- Launch the UI by running `python sekernel_ui.py`

## Short-films
https://github.com/user-attachments/assets/a6e75136-bd3f-4960-8791-6f83094f2123

