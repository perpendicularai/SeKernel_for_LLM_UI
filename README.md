# SeKernel_for_LLM_UI
This is the repository for the UI for the SeKernel_for_LLM module

## How to:
- Clone the repo
- Ensure that you have llama-cpp-python installed and running
- Add your model to the `kernel.py` script
- Launch the UI by running `python sekernel_ui.py`
